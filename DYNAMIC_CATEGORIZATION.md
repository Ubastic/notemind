# AI 动态分类与目录重构方案 (AI Dynamic Categorization)

## 1. 核心理念 (Core Concept)
目前的分类（Category）是预设固定的（如 Work, Personal, Idea），而新的构想是**"流动的知识图谱"**。
系统不再强制用户去适配分类，而是分类去适配用户的笔记内容。
- **两级结构**：大分类 (Category) -> 小目录 (Folder/Topic)。
- **动态生成**：由大模型基于笔记内容的语义聚类生成。
- **人机协作**：AI 提出建议，用户进行最终的“园艺修剪”（拖拽、重命名、锁定）。

## 2. 产品与设计 (Product & Design)

### 2.1 交互流程
1. **冷启动/全量整理**：
   - 用户点击“AI 整理我的大脑”按钮。
   - AI 分析所有笔记，根据语义相似度聚类。
   - 重新划分 Categories 和 Folders。
   - **关键点**：提供“预览/差异对比”模式，展示“这 10 个笔记将被移动到新建立的‘React 学习’目录下”，用户确认后生效。

2. **日常使用 (增量整理)**：
   - 新建笔记时，AI 默认归入“Inbox/未分类”。
   - 或者 AI 自动判断它属于现有的某个动态分类，并打上 `Suggested` 标记。
   - 用户在侧边栏看到的分类列表是动态的。

3. **人工干预 (Gardening)**：
   - **拖拽**：用户将笔记从 "AI 聚类 A" 拖到 "AI 聚类 B"。
   - **重命名**：AI 起名太啰嗦？用户改为简短名字。
   - **锁定 (Pin/Lock)**：用户可以“钉住”某个分类，告诉 AI “这个分类是我定的，以后不要动它，只能往里加东西，不能拆散它”。

### 2.2 视觉设计
- **动态侧边栏**：不再是死板的列表，可能是一个可以展开/折叠的树状结构，或者类似 macOS 的“智能文件夹”。
- **聚类视图 (Cluster View)**：一个二维画布，展示笔记的分布簇，直观看到哪些笔记是一类的。
- **操作反馈**：当 AI 建议移动笔记时，使用虚线框或高亮提示。

## 3. 技术实现 (Technical Implementation)

### 3.1 核心算法
1. **Embedding (向量化)**：
   - 现有的 `Note.embedding` 字段存储笔记内容的向量表示。
   - 使用 OpenAI `text-embedding-3-small` 或本地模型。

2. **Clustering (聚类)**：
   - **算法选择**：K-Means (需要指定 K，不灵活) vs **HDBSCAN** / **Agglomerative Clustering** (层次聚类，更适合自动发现数量不定的类目)。
   - **两级聚类**：
     - 第一层聚类生成 **Category**。
     - 在每个 Category 内部再次聚类生成 **Folder**。

3. **Naming (命名)**：
   - 聚类完成后，抽取每个簇中心附近的几篇笔记。
   - 发送给 LLM (Qwen/GPT)："这几篇笔记是关于什么的？请给出一个简短的分类名称（5个字以内）"。

### 3.2 数据结构调整
目前 `Note` 表已有 `ai_category` 和 `folder` 字段，通过字符串存储。
为了支持动态管理，建议：
- **方案 A (轻量级)**：继续使用字符串字段。AI 只是批量更新这些字段。
- **方案 B (结构化)**：引入 `Taxonomy` 存储结构。
  ```json
  // UserSettings.taxonomy_state
  {
    "categories": [
      {
        "id": "cat_1",
        "name": "前端开发",
        "is_locked": true, // 用户锁定，AI 不予调整
        "folders": [
          { "id": "fold_1", "name": "React" },
          { "id": "fold_2", "name": "Vue" }
        ]
      }
    ]
  }
  ```

### 3.3 性能优化与低资源适配 (Low Resource Adaptation)
由于主机配置极低 (单核/双核, <100MB RAM)，传统的 `scikit-learn` + 全量 Embedding 方案不可行。建议采用以下轻量化替代方案：

#### 方案 A: LLM 直接分类 (最轻量，推荐)
完全放弃本地聚类算法，将计算压力转移到 LLM。
1. **流程**：
   - 后端查询出所有待分类笔记的 `(id, title, summary)`。
   - 组装 Prompt：`"我有一组笔记，请根据内容将它们整理成 5-10 个合理的分类。返回 JSON 格式：{category: [note_id, ...]}"`。
   - 调用 LLM API (OpenAI/Qwen/DeepSeek)。
   - 解析返回的 JSON，更新数据库。
2. **优点**：本地内存占用极低（仅需处理字符串），语义理解能力最强。
3. **缺点**：Token 消耗量大，笔记量大时需分批处理（Map-Reduce 模式）。

#### 方案 B: Embedding API + 贪心聚类 (Leader-Follower Algorithm)
利用 Embedding API 获取向量，但在本地使用极简算法，不依赖 scikit-learn。
1. **向量获取**：调用 OpenAI/DashScope Embedding API，不要在本地跑 Embedding 模型。
2. **流式聚类算法 (纯 Python 实现)**：
   - 维护一个 `Clusters` 列表，每个 Cluster 记录一个中心向量 `Centroid`。
   - 遍历每篇笔记：
     - 计算它与现有所有 `Centroid` 的余弦相似度。
     - 如果最大相似度 > 阈值 (如 0.8)，归入该 Cluster，并更新 Cluster 中心（加权平均）。
     - 否则，创建一个新 Cluster。
3. **资源消耗**：
   - 不需要加载 huge matrix。
   - 内存仅需存储 `K` 个中心向量和当前处理的 1 个向量。
   - 可以在 100MB 内存内轻松处理数千篇笔记。

### 3.4 大规模数据适配 (Scalability for Large Datasets)
如果笔记数量巨大 (e.g. > 1000 篇)，方案 A (全量 LLM) 会遇到 Context Window 限制和高昂成本。
建议采用 **"分层混合策略" (Map-Reduce 思想)**：
1.  **第一步：本地粗聚类 (Map)**
    - 使用 **方案 B (Leader-Follower)** 在本地快速将 1000 篇笔记聚成 50-100 个“微簇 (Micro-Clusters)”。
    - 这一步不求精准，只求把明显相似的放在一起。
    - 提取每个微簇的 3 个代表性标题。

2.  **第二步：LLM 构建架构 (Reduce)**
    - 只将这 50-100 个微簇的**代表标题** (数据量缩小 90%) 发送给 LLM。
    - Prompt: `"这里有 50 组话题，请将它们归纳合并为 8-10 个顶级分类树。"`
    - LLM 返回分类结构 (Category -> Folders)。

3.  **第三步：本地归位**
    - 根据 LLM 返回的结构，将微簇内的所有笔记批量移动到对应的目标分类中。

4.  **增量更新机制**
    - 新笔记创建时，先在本地计算它与现有分类中心的相似度。
    - 如果 > 阈值，直接归入。
    - 如果 < 阈值，放入 "Inbox/待整理"。
    - 当 "Inbox" 积攒到 20 篇时，仅对这 20 篇触发一次小型 LLM 整理。

### 3.5 任务调度

## 4. 潜在挑战与对策
1. **不稳定性**：用户今天看到是分类 A，明天 AI 把它改成了分类 B，会造成困扰。
   - **对策**：增加“惯性”。除非语义发生巨大漂移，否则尽量保持现有分类不变。只有用户显式点击“重新整理”时才大动干戈。
2. **分类碎片化**：出现太多只有一个笔记的分类。
   - **对策**：设置聚类的最小阈值（例如至少 3 篇笔记才能成一个 Folder），否则归入“其他”。

## 5. 后续开发路线图 (Roadmap)
1. **Phase 1**: 后端实现 `/api/ai/organize` 接口，支持基于 Embedding 的全量聚类并返回建议结构。
2. **Phase 2**: 前端实现“整理预览”弹窗，允许用户接受或拒绝 AI 的建议。
3. **Phase 3**: 实现拖拽交互与“锁定分类”逻辑，完善人机协作流。
